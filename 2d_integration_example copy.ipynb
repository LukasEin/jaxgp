{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recovering a 2d function from its gradient with a Gaussian Process Regression model\n",
    "\n",
    "This notebook shows examples on how to use the present gaussian process regression framework to formally integrate functions from derivative observations.\n",
    "\n",
    "This example will show how to predict a 2D function using gradient observations with both a full GPR and a sparse GPR. The sparse GPR framework works by projecting the training data into a lower dimensional feature space in order to reduce the computational cost (mainly matrix inversions)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training data set\n",
    "\n",
    "`jax.numpy` has almost the same usage as the standard `numpy` package, with the caveat that `jax.ndarray` is an immutable type, meaning that no inplace changes can be made. For creating training data this should however not be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.scipy as jsp\n",
    "from jax.lax import Precision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will model a forth order polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "def fun(x, noise=0.0, key = random.PRNGKey(0)):\n",
    "    return (x[:,0]**2 + x[:,1] - 11)**2 / 800.0 + (x[:,0] + x[:,1]**2 -7)**2 / 800.0 + random.normal(key,(len(x),), dtype=jnp.float32)*noise\n",
    "\n",
    "def grad(x, noise=0.0, key = random.PRNGKey(0)):\n",
    "    dx1 = 4 * (x[:,0]**2 + x[:,1] - 11) * x[:,0] + 2 * (x[:,0] + x[:,1]**2 -7)\n",
    "    dx2 = 2 * (x[:,0]**2 + x[:,1] - 11) + 4 * (x[:,0] + x[:,1]**2 -7) * x[:,1]\n",
    "    return jnp.vstack((dx1, dx2)).T / 800.0 + random.normal(key,x.shape, dtype=jnp.float32)*noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the training data we first need to define boundaries to choose the datapoints from. Then, random points are chosen in this interval. `random.split` creates a new subkey from the previous key to get a new sudo-random sample from the uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interval bounds from which to choose the data points\n",
    "bounds = jnp.array([-5.0, 5.0])\n",
    "\n",
    "# How many function and derivative observations should be chosen\n",
    "num_f_vals = 1\n",
    "num_d_vals = 100\n",
    "\n",
    "# initial seed for the pseudo random key generation\n",
    "seed = 0\n",
    "\n",
    "# create new keys and randomly sample the above interval for training features\n",
    "key, subkey = random.split(random.PRNGKey(seed))\n",
    "x_func = random.uniform(subkey, (num_f_vals, 2), minval=bounds[0], maxval=bounds[1])\n",
    "key, subkey = random.split(key)\n",
    "x_der = random.uniform(subkey, (num_d_vals,2), minval=bounds[0], maxval=bounds[1])\n",
    "\n",
    "# noise with which to sample the training labels\n",
    "noise = 0.02\n",
    "key, subkey = random.split(key)\n",
    "y_func = fun(x_func,noise, subkey)\n",
    "key, subkey = random.split(key)\n",
    "y_der = grad(x_der, noise, subkey)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPR framework needs as input for training a tuple of arrays `X_split` of which contains a set of points where the function is sampled and a set of points where the gradient is sampled. Both array in `X_split` is of shape `(n_samples_i, N)`. `X_split` should be ordered as follows: the first array represents the datapoints for the function observations and the second array represents the gradient of the function. `Y_train` should just be an array of shape `(n_samples_function + n_samples_gradient,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_split = [x_func,x_der]\n",
    "\n",
    "Y_train = jnp.hstack((y_func, y_der.reshape(-1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Kernel and its initial parameters\n",
    "\n",
    "The kernels can be found in `jaxgp.kernels`. Currently implemented are `RBF`, `Linear`, and `Periodic` kernels. When in doubt what kernel to use, go with an `RBF` kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.kernels import RBF\n",
    "\n",
    "kernel = RBF()\n",
    "# an RBF kernel has per default 2 parameters\n",
    "init_kernel_params = jnp.array([2.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.regression import ExactGPR\n",
    "from jaxgp.utils import Logger\n",
    "\n",
    "logger = Logger()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sparse GPR model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the sparse GPR model\n",
    "\n",
    "The `sparseGPR` model can be found in `jaxgp.regression`. The idea of a sparse model is to project your training data into a space with smaller dimension in order to save in computational cost. This is done by projecting your full training set onto a set of reference points via the kernel. \n",
    "\n",
    "There are typically 2 methods to choose reference points:\n",
    " - choosing a subset of size $m<n$ from the existing datapoints\n",
    " - creating an even grid inside the bounds on which the model should be evaluated\n",
    "\n",
    "Note that below the evenly spread reference grid has fewer points than the subset grid. This was done in order to get a nice even grid which was achieved by taking the largerst perfect square smaller than the number of reference points.\n",
    "\n",
    "Furthermore we also created a larger set of gradient observations since the predictive power of the sparse model is lower than of the full model. However, this is not a problem computation wise as is seen further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.regression import SparseGPR\n",
    "\n",
    "# How many function and derivative observations should be chosen\n",
    "num_f_vals = 1\n",
    "num_d_vals = 1000\n",
    "\n",
    "# initial seed for the pseudo random key generation\n",
    "seed = 0\n",
    "\n",
    "# create new keys and randomly sample the above interval for training features\n",
    "key, subkey = random.split(random.PRNGKey(seed))\n",
    "x_func = random.uniform(subkey, (num_f_vals, 2), minval=bounds[0], maxval=bounds[1])\n",
    "key, subkey = random.split(key)\n",
    "x_der = random.uniform(subkey, (num_d_vals,2), minval=bounds[0], maxval=bounds[1])\n",
    "\n",
    "# noise with which to sample the training labels\n",
    "noise = 0.02\n",
    "key, subkey = random.split(key)\n",
    "y_func = fun(x_func,noise, subkey)\n",
    "key, subkey = random.split(key)\n",
    "y_der = grad(x_der, noise, subkey)\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "num_ref_points = (num_d_vals + num_f_vals) // 20\n",
    "key, subkey = random.split(key)\n",
    "X_ref_rand = random.permutation(subkey, jnp.vstack((x_der,x_func)))[:num_ref_points]\n",
    "# this grid has fewer points. It has N points, where N is the largest perfect square smaller than num_ref_points\n",
    "X_ref_even = jnp.array(jnp.meshgrid(jnp.linspace(*bounds, round(jnp.sqrt(num_ref_points))),jnp.linspace(*bounds, round(jnp.sqrt(num_ref_points))))).reshape(2,-1).T\n",
    "\n",
    "model_rand = SparseGPR(kernel, init_kernel_params, noise, X_ref_rand, logger=None)\n",
    "model_even = SparseGPR(kernel, init_kernel_params, noise, X_ref_even, logger=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_split = [x_func,x_der]\n",
    "\n",
    "Y_train = jnp.hstack((y_func, y_der.reshape(-1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the time needed to train the sparse models, even with 10 times as many datapoints the computation is still twice as fast, compare to the full model. This is because the computational effort is roughly 100 times smaller: \n",
    " - The full model needs $\\mathcal{O}(N^3)$ flops to train and fit the model\n",
    " - The sparse model needs $\\mathcal{O}(M^2N + M^3)$ flops to train and fit the model\n",
    "\n",
    "which comes out to roughly a factor 100 times faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptStep(params=DeviceArray([2., 2.], dtype=float32), state=ScipyMinimizeInfo(fun_val=DeviceArray(nan, dtype=float32, weak_type=True), success=False, status=2, iter_num=0))\n",
      "OptStep(params=DeviceArray([0.00379112, 0.33245406], dtype=float32), state=ScipyMinimizeInfo(fun_val=DeviceArray(-1057.3768, dtype=float32, weak_type=True), success=True, status=0, iter_num=24))\n"
     ]
    }
   ],
   "source": [
    "model_rand.train(X_split, Y_train)\n",
    "model_even.train(X_split, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jaxgp.likelyhood import sparse_kernelNegativeLogLikelyhood\n",
    "\n",
    "param_bounds = jnp.array([1e-3,10.0])\n",
    "num_params = 20\n",
    "params = jnp.array(jnp.meshgrid(jnp.linspace(*param_bounds, num_params),jnp.linspace(*param_bounds, num_params))).reshape(2,-1).T\n",
    "\n",
    "mapping = jax.vmap(lambda x: jax.jit(sparse_kernelNegativeLogLikelyhood)(x, X_split, Y_train, X_ref_even, noise, kernel), in_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlle ="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
