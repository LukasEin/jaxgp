{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training data set\n",
    "\n",
    "`jax.numpy` has almost the same usage as the standard `numpy` package, with the caveat that `jax.ndarray` is an immutable type, meaning that no inplace changes can be made. For creating training data this should however not be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will model a forth order polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x, noise=0.0, key = random.PRNGKey(0)):\n",
    "    return (x[:,0]**2 + x[:,1] - 11)**2 / 800.0 + (x[:,0] + x[:,1]**2 -7)**2 / 800.0 + random.normal(key,(len(x),), dtype=jnp.float32)*noise\n",
    "\n",
    "def grad(x, noise=0.0, key = random.PRNGKey(0)):\n",
    "    dx1 = 4 * (x[:,0]**2 + x[:,1] - 11) * x[:,0] + 2 * (x[:,0] + x[:,1]**2 -7)\n",
    "    dx2 = 2 * (x[:,0]**2 + x[:,1] - 11) + 4 * (x[:,0] + x[:,1]**2 -7) * x[:,1]\n",
    "    return jnp.vstack((dx1, dx2)).T / 800.0 + random.normal(key,x.shape, dtype=jnp.float32)*noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the training data we first need to define boundaries to choose the datapoints from. Then, random points are chosen in this interval. `random.split` creates a new subkey from the previous key to get a new sudo-random sample from the uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interval bounds from which to choose the data points\n",
    "bounds = jnp.array([-5.0, 5.0])\n",
    "\n",
    "# How many function and derivative observations should be chosen\n",
    "num_f_vals = 1\n",
    "num_d_vals = 50\n",
    "\n",
    "# initial seed for the pseudo random key generation\n",
    "seed = 0\n",
    "\n",
    "# create new keys and randomly sample the above interval for training features\n",
    "key, subkey = random.split(random.PRNGKey(seed))\n",
    "x_func = random.uniform(subkey, (num_f_vals, 2), minval=bounds[0], maxval=bounds[1])\n",
    "key, subkey = random.split(key)\n",
    "x_der = random.uniform(subkey, (num_d_vals,2), minval=bounds[0], maxval=bounds[1])\n",
    "\n",
    "# noise with which to sample the training labels\n",
    "noise = 0.1\n",
    "key, subkey = random.split(key)\n",
    "y_func = fun(x_func,noise, subkey)\n",
    "key, subkey = random.split(key)\n",
    "y_der = grad(x_der, noise, subkey)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPR framework needs as input for training a tuple of arrays `X_split` of which contains a set of points where the function is sampled and a set of points where the gradient is sampled. Both array in `X_split` is of shape `(n_samples_i, N)`. `X_split` should be ordered as follows: the first array represents the datapoints for the function observations and the second array represents the gradient of the function. `Y_train` should just be an array of shape `(n_samples_function + n_samples_gradient,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_split = [x_func,x_der]\n",
    "\n",
    "Y_train = (y_func.reshape(-1,1), y_der.reshape(-1,1)) # jnp.hstack((y_func, y_der.reshape(-1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before BayesOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.kernels import RBF\n",
    "\n",
    "kernel = RBF()\n",
    "# an RBF kernel has per default 2 parameters\n",
    "init_kernel_params = jnp.array([2.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.regression import ExactGPR\n",
    "from jaxgp.utils import Logger\n",
    "\n",
    "logger = Logger()\n",
    "model = ExactGPR(kernel, init_kernel_params, noise, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(X_split, jnp.vstack(Y_train).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_grid = jnp.array(jnp.meshgrid(jnp.linspace(*bounds, 101), jnp.linspace(*bounds, 101))).T.reshape(101**2, 2)\n",
    "\n",
    "# model.eval returns a mean prediction and a confidence interval around the mean prediction\n",
    "means, stds = model.eval(predict_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "means = means.reshape(101,101)\n",
    "stds = stds.reshape(101,101)\n",
    "true = fun(predict_grid).reshape(101,101)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10,7))\n",
    "\n",
    "mesh = jnp.meshgrid(jnp.linspace(*bounds, 101),jnp.linspace(*bounds, 101))\n",
    "\n",
    "im1 = ax[0,0].contourf(*mesh, means, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im2 = ax[0,1].contourf(*mesh, true, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im3 = ax[1,0].contourf(*mesh, stds)\n",
    "im4 = ax[1,1].contourf(*mesh, jnp.abs(means-true))\n",
    "# im4 = ax[1,1].contourf(*mesh, jnp.greater(true, means-stds)*jnp.less(true, means+stds))\n",
    "\n",
    "ax[1,1].scatter(X_split[1][:,0], X_split[1][:,1], c=\"pink\", marker=\"x\", label=\"der pos\")\n",
    "ax[1,0].scatter(X_split[1][:,0], X_split[1][:,1], c=\"pink\", marker=\"x\", label=\"der pos\")\n",
    "\n",
    "ax[1,1].scatter(x_der[:,0], x_der[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "ax[1,1].scatter(x_func[:,0], x_func[:,1], c=\"orange\", marker=\"+\", label=\"fun pos\")\n",
    "\n",
    "ax[1,0].scatter(x_der[:,0], x_der[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "ax[1,0].scatter(x_func[:,0], x_func[:,1], c=\"orange\", marker=\"+\", label=\"fun pos\")\n",
    "\n",
    "plt.colorbar(im1, ax=ax[0,0])\n",
    "plt.colorbar(im2, ax=ax[0,1])\n",
    "plt.colorbar(im3, ax=ax[1,0])\n",
    "plt.colorbar(im4, ax=ax[1,1])\n",
    "\n",
    "ax[0,0].set_title(\"prediction\")\n",
    "ax[0,1].set_title(\"true function\")\n",
    "ax[1,0].set_title(\"std\")\n",
    "ax[1,1].set_title(\"abs dif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_before = jnp.mean(jnp.abs(means-true))\n",
    "mse_before = jnp.mean(jnp.abs(means-true)**2)\n",
    "\n",
    "print(f\"{mae_before=}, {mse_before=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.bayesopt import *\n",
    "\n",
    "rand = 3\n",
    "bayesopt_bounds = jnp.array([[-5.0,-5.0],[5.0,5.0]])\n",
    "# eval_func = lambda x: grad(x, noise, key).reshape(-1,1)\n",
    "eval_func = lambda x: grad(x).reshape(-1,1)\n",
    "explore_param = 2\n",
    "grid = jnp.array(jnp.meshgrid(jnp.linspace(*bounds, 101), jnp.linspace(*bounds, 101))).T.reshape(101**2, 2)\n",
    "\n",
    "acqui_fun = UpperConfidenceBound(grid, explore_param)\n",
    "# acqui_fun = MaximumVariance(grid)\n",
    "\n",
    "bayesopt = ExactBayesOpt(X_split, Y_train, kernel, acquisition_func=acqui_fun, eval_func=eval_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesopt(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_split, Y_train = bayesopt.X_split, bayesopt.Y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After BayesOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.regression import ExactGPR\n",
    "from jaxgp.utils import Logger\n",
    "\n",
    "logger = Logger()\n",
    "model = ExactGPR(kernel, init_kernel_params, noise, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(X_split, jnp.vstack(Y_train).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_grid = jnp.array(jnp.meshgrid(jnp.linspace(*bounds, 101), jnp.linspace(*bounds, 101))).T.reshape(101**2, 2)\n",
    "\n",
    "# model.eval returns a mean prediction and a confidence interval around the mean prediction\n",
    "means, stds = model.eval(predict_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "means = means.reshape(101,101)\n",
    "stds = stds.reshape(101,101)\n",
    "true = fun(predict_grid).reshape(101,101)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10,7))\n",
    "\n",
    "mesh = jnp.meshgrid(jnp.linspace(*bounds, 101),jnp.linspace(*bounds, 101))\n",
    "\n",
    "im1 = ax[0,0].contourf(*mesh, means, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im2 = ax[0,1].contourf(*mesh, true, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im3 = ax[1,0].contourf(*mesh, stds)\n",
    "im4 = ax[1,1].contourf(*mesh, jnp.abs(means-true))\n",
    "# im4 = ax[1,1].contourf(*mesh, jnp.greater(true, means-stds)*jnp.less(true, means+stds))\n",
    "\n",
    "ax[1,1].scatter(X_split[1][:,0], X_split[1][:,1], c=\"pink\", marker=\"x\", label=\"der pos\")\n",
    "ax[1,0].scatter(X_split[1][:,0], X_split[1][:,1], c=\"pink\", marker=\"x\", label=\"der pos\")\n",
    "\n",
    "ax[1,1].scatter(x_der[:,0], x_der[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "ax[1,1].scatter(x_func[:,0], x_func[:,1], c=\"orange\", marker=\"+\", label=\"fun pos\")\n",
    "\n",
    "ax[1,0].scatter(x_der[:,0], x_der[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "ax[1,0].scatter(x_func[:,0], x_func[:,1], c=\"orange\", marker=\"+\", label=\"fun pos\")\n",
    "\n",
    "plt.colorbar(im1, ax=ax[0,0])\n",
    "plt.colorbar(im2, ax=ax[0,1])\n",
    "plt.colorbar(im3, ax=ax[1,0])\n",
    "plt.colorbar(im4, ax=ax[1,1])\n",
    "\n",
    "ax[0,0].set_title(\"prediction\")\n",
    "ax[0,1].set_title(\"true function\")\n",
    "ax[1,0].set_title(\"std\")\n",
    "ax[1,1].set_title(\"abs dif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_after = jnp.mean(jnp.abs(means-true))\n",
    "mse_after = jnp.mean(jnp.abs(means-true)**2)\n",
    "\n",
    "print(f\"{mae_after=}, {mse_after=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
