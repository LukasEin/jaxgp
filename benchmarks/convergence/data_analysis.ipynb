{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "import make_df\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    return (x[:,0]**2 + x[:,1] - 11)**2 / 800.0 + (x[:,0] + x[:,1]**2 -7)**2 / 800.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsification\n",
    "sparse = True\n",
    "mode = \"rand\"\n",
    "subset_size = 0.02\n",
    "sparse_dict = {0.1: \"log_sparse_0_1\", 0.02: \"log_sparse_0_02\", 0.3: \"log_sparse_0_3\", 0.005: \"log_sparse_0_005\"}\n",
    "\n",
    "if sparse:\n",
    "    # directory where to save stuff\n",
    "    in_dir = f\"./sparse/{mode}/{sparse_dict[subset_size]}\"\n",
    "\n",
    "    # How many derivative observations should be chosen\n",
    "    list_d_vals = [200, 400, 800, 1500, 2000, 3000]\n",
    "else:\n",
    "    # directory where to save stuff\n",
    "    in_dir = \"./full\"\n",
    "    # How many derivative observations should be chosen\n",
    "    list_d_vals = [5, 20, 50, 100, 200, 400, 800]\n",
    "\n",
    "\n",
    "# optimizer type\n",
    "optimizers = [\"L-BFGS-B\", \"TNC\", \"SLSQP\"]\n",
    "\n",
    "# name of the function\n",
    "name = \"him\"\n",
    "\n",
    "# How many function observations should be chosen\n",
    "list_f_vals = [1, 5, 20, 50]\n",
    "\n",
    "# Grid on which to evaluate the function\n",
    "bounds = jnp.array([[-5.0, -5.0], [5.0, 5.0]])\n",
    "eval_grid = jnp.linspace(bounds[0], bounds[1],100).T\n",
    "eval_grid = jnp.array(jnp.meshgrid(*eval_grid)).reshape(2,-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_df.make_df(list_f_vals, list_d_vals, optimizers, in_dir, name, sparse, subset_size, fun, eval_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare MSE for different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"b\", \"r\", \"g\", \"cyan\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "for i, optimizer in enumerate(optimizers):\n",
    "    opt_data = data[data[\"opt\"] == optimizer]\n",
    "    for c,f in zip(colors, opt_data[\"f\"].unique()):\n",
    "        temp = opt_data[opt_data[\"f\"] == f]\n",
    "        dvals = temp[\"d\"]\n",
    "\n",
    "        # mean_mse = temp[\"mean_mse\"]\n",
    "        median_mse = temp[\"median_mse\"]\n",
    "        min_mse = temp[\"min_mse\"]\n",
    "        max_mse = temp[\"max_mse\"]\n",
    "        # ax[i].plot(dvals, mean_mse, color=c, ls=\"--\", marker=\"x\", label=f\"#f-vals = {f}\")\n",
    "        ax[i].plot(dvals, median_mse, color=c, ls=\"--\", marker=\"x\", label=f\"#f-vals = {f}\")\n",
    "        ax[i].fill_between(dvals, min_mse.tolist(), max_mse.tolist(), color=c, alpha=0.2)\n",
    "        ax[i].set_yscale(\"log\")\n",
    "        ax[i].set_xscale(\"log\")\n",
    "\n",
    "\n",
    "    ax[i].grid()\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel(\"#d-vals\")\n",
    "    ax[i].set_title(optimizer)\n",
    "\n",
    "ax[0].set_ylabel(\"MSE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare percentage of True function in $1\\sigma$ confidence interval for different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"b\", \"r\", \"g\", \"cyan\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "for i, optimizer in enumerate(optimizers):\n",
    "    opt_data = data[data[\"opt\"] == optimizer]\n",
    "    for c,f in zip(colors, opt_data[\"f\"].unique()):\n",
    "        temp = opt_data[opt_data[\"f\"] == f]\n",
    "        dvals = temp[\"d\"]\n",
    "\n",
    "        mean_tic = temp[\"mean_tic\"]\n",
    "        min_tic = temp[\"min_tic\"]\n",
    "        max_tic = temp[\"max_tic\"]\n",
    "        ax[i].plot(dvals, mean_tic, color=c, ls=\"--\", marker=\"x\", label=f\"#f-vals = {f}\")\n",
    "        ax[i].fill_between(dvals, min_tic.tolist(), max_tic.tolist(), color=c, alpha=0.2)\n",
    "\n",
    "\n",
    "    ax[i].grid()\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel(\"#d-vals\")\n",
    "    ax[i].set_title(optimizer)\n",
    "ax[0].set_ylabel(\"% Y inside conf.-interval\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare predicted error for different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"b\", \"r\", \"g\", \"cyan\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "for i, optimizer in enumerate(optimizers):\n",
    "    opt_data = data[data[\"opt\"] == optimizer]\n",
    "    for c,f in zip(colors, opt_data[\"f\"].unique()):\n",
    "        temp = opt_data[opt_data[\"f\"] == f]\n",
    "        dvals = temp[\"d\"]\n",
    "\n",
    "        mean_tic = temp[\"median_maxerrs\"]\n",
    "        min_tic = temp[\"min_maxerrs\"]\n",
    "        max_tic = temp[\"max_maxerrs\"]\n",
    "        ax[i].plot(dvals, mean_tic, color=c, ls=\"--\", marker=\"x\", label=f\"#f-vals = {f}\")\n",
    "        ax[i].fill_between(dvals, min_tic.tolist(), max_tic.tolist(), color=c, alpha=0.2)\n",
    "        ax[i].set_yscale(\"log\")\n",
    "        ax[i].set_xscale(\"log\")\n",
    "\n",
    "\n",
    "    ax[i].grid()\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel(\"#d-vals\")\n",
    "    ax[i].set_title(optimizer)\n",
    "ax[0].set_ylabel(\"predicted standard error\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between different sparsifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"b\", \"r\", \"g\", \"cyan\"]\n",
    "list_d_vals = [200, 400, 800, 1500, 2000, 3000]\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 8), sharey=True)\n",
    "\n",
    "sparse_list = [[0.005, 0.02], [0.1, 0.3]]\n",
    "optimizers = [\"SLSQP\"]\n",
    "\n",
    "for i, elem in enumerate(sparse_list):\n",
    "    for j, sparse in enumerate(elem):\n",
    "        in_dir = f\"./sparse/{mode}/{sparse_dict[sparse]}\"\n",
    "        data = make_df.make_df(list_f_vals, list_d_vals, optimizers, in_dir, name, True, sparse, fun, eval_grid)\n",
    "        opt_data = data[data[\"opt\"] == optimizers[0]]\n",
    "        for c,f in zip(colors, opt_data[\"f\"].unique()):\n",
    "            temp = opt_data[opt_data[\"f\"] == f]\n",
    "            dvals = temp[\"d\"]\n",
    "\n",
    "            # mean_mse = temp[\"mean_mse\"]\n",
    "            median_mse = temp[\"median_mse\"]\n",
    "            min_mse = temp[\"min_mse\"]\n",
    "            max_mse = temp[\"max_mse\"]\n",
    "            # ax[i].plot(dvals, mean_mse, color=c, ls=\"--\", marker=\"x\", label=f\"#f-vals = {f}\")\n",
    "            ax[i,j].plot(dvals, median_mse, color=c, ls=\"--\", marker=\"x\", label=f\"#f-vals = {f}\")\n",
    "            ax[i,j].fill_between(dvals, min_mse.tolist(), max_mse.tolist(), color=c, alpha=0.2)\n",
    "            ax[i,j].set_yscale(\"log\")\n",
    "            ax[i,j].set_xscale(\"log\")\n",
    "\n",
    "\n",
    "        ax[i,j].grid()\n",
    "        # ax[i,j].legend()\n",
    "        ax[i,j].set_title(sparse)\n",
    "\n",
    "ax[1,0].set_xlabel(\"#d-vals\")\n",
    "ax[1,1].set_xlabel(\"#d-vals\")\n",
    "ax[0,0].set_ylabel(\"MSE\")\n",
    "ax[1,0].set_ylabel(\"MSE\")\n",
    "fig.suptitle(\"MSE over training set size for different sparsification\")\n",
    "handles, labels = ax[0,0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
