{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recovering a 1d function from its gradient with a Gaussian Process Regression model\n",
    "\n",
    "This notebook shows examples on how to use the present gaussian process regression framework to formally integrate gradient observations of a function.\n",
    "\n",
    "This example will show how to predict a 1D function using gradient information with both a full GPR and a sparse GPR. The sparse GPR framework works by projecting the training data into a lower dimensional feature space in order to reduce the computational cost (mainly matrix inversions)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training data set\n",
    "\n",
    "`jax.numpy` has almost the same usage as the standard `numpy` package, with the caveat that `jax.ndarray` is an immutable type, meaning that no inplace changes can be made. For creating training data this should however not be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will model a simple sin function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true function is a noisy lennard jones potential\n",
    "def sin(x, noise=0.0, key = random.PRNGKey(0)):\n",
    "    return jnp.sin(x) + random.normal(key,x.shape, dtype=jnp.float32)*noise\n",
    "\n",
    "def cos(x, noise=0.0, key = random.PRNGKey(0)):\n",
    "    return jnp.cos(x) + random.normal(key,x.shape, dtype=jnp.float32)*noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the training data we first need to define boundaries to choose the datapoints from. Then, random points are chosen in this interval. `random.split` creates a new subkey from the previous key to get a new sudo-random sample from the uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interval bounds from which to choose the data points\n",
    "bounds = jnp.array([0.0, 2*jnp.pi])\n",
    "\n",
    "# How many function and derivative observations should be chosen\n",
    "num_f_vals = (1,)\n",
    "num_d_vals = (10,)\n",
    "\n",
    "# initial seed for the pseudo random key generation\n",
    "seed = 0\n",
    "\n",
    "# create new keys and randomly sample the above interval for training features\n",
    "key, subkey = random.split(random.PRNGKey(seed))\n",
    "x_func = random.uniform(subkey, num_f_vals, minval=bounds[0], maxval=bounds[1])\n",
    "key, subkey = random.split(key)\n",
    "x_der = random.uniform(subkey, num_d_vals, minval=bounds[0], maxval=bounds[1])\n",
    "\n",
    "# noise with which to sample the training labels\n",
    "noise = 0.1\n",
    "key, subkey = random.split(key)\n",
    "y_func = sin(x_func,noise, subkey)\n",
    "key, subkey = random.split(key)\n",
    "y_der = cos(x_der, noise, subkey)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPR framework needs as input for training a tuple of arrays `X_split` of which contains a set of points where the function is sampled and a set of points where the gradient is sampled. Both array in `X_split` is of shape `(n_samples_i, N)`. `X_split` should be ordered as follows: the first array represents the datapoints for the function observations and the second array represents the gradient of the function. `Y_train` should just be an array of shape `(n_samples_function + n_samples_gradient,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping needs to be done the get the arrays in the form (n_samples_i, N)\n",
    "X_split = [x_func.reshape(-1,1),x_der.reshape(-1,1)]\n",
    "\n",
    "Y_train = jnp.hstack((y_func, y_der))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Kernel and its initial parameters\n",
    "\n",
    "The kernels can be found in `jaxgp.kernels`. Currently implemented are `RBF`, `Linear`, and `Periodic` kernels. When in doubt what kernel to use, go with an `RBF` kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.kernels import RBF\n",
    "\n",
    "kernel = RBF()\n",
    "# an RBF kernel has per default 2 parameters\n",
    "init_kernel_params = jnp.ones((2,))*jnp.log(2.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The full GPR model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the full GPR model\n",
    "\n",
    "The regression models can be found in `jaxgp.regression`. The `ExactGPR` model takes as inital arguments a kernel to use, initial parameters for the kernel and a noise parameter that underlies the noisy datageneration.\n",
    "\n",
    "Additionally one can also give the model a logging function that saves the parameters at each step of the optimization. For this a convenience class `Logger` can be found in `jaxgp.utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.regression import ExactGPR\n",
    "from jaxgp.utils import Logger\n",
    "\n",
    "logger = Logger()\n",
    "model = ExactGPR(kernel, init_kernel_params, noise, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(X_split, Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterations of the parameters during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logger.iters_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with the trained model\n",
    "\n",
    "We will make predictions on an evenly spaced grid of 200 points in the above defined boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_grid = jnp.linspace(*bounds, 200)\n",
    "\n",
    "# model.eval returns a mean prediction and a confidence interval around the mean prediction\n",
    "means, stds = model.eval(predict_grid.reshape(-1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the result\n",
    "\n",
    "Plotting the predicted function together with its confidence interval we can see that even with only a few datapoints the prediction works good. The shaded region describes the $1\\sigma$ confidence interval around the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(predict_grid, means, label=\"prediction\")\n",
    "plt.fill_between(predict_grid, means-stds, means+stds, alpha=0.5)\n",
    "\n",
    "plt.plot(predict_grid, sin(predict_grid), c=\"gray\", ls=\"--\",label=\"true function\")\n",
    "\n",
    "plt.scatter(x_func, y_func, c=\"r\", label=\"function eval\")\n",
    "for i,x in enumerate(x_der): \n",
    "    if i == 0:\n",
    "        plt.axvline(x, c=\"r\", lw=0.8, ls=\"--\", label=\"deriv positions\")\n",
    "    else:\n",
    "        plt.axvline(x, c=\"r\", lw=0.8, ls=\"--\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sparse GPR model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the sparse GPR model\n",
    "\n",
    "The `sparseGPR` model can be found in `jaxgp.regression`. The idea of a sparse model is to project your training data into a space with smaller dimension in order to save in computational cost. This is done by projecting your full training set onto a set of reference points via the kernel. \n",
    "\n",
    "There are typically 2 methods to choose reference points:\n",
    " - choosing a subset of size $m<n$ from the existing datapoints\n",
    " - creating an even grid inside the bounds on which the model should be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.regression import SparseGPR\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "num_ref_points = (3,)\n",
    "key, subkey = random.split(key)\n",
    "X_ref_rand = random.choice(subkey, x_der, num_ref_points)\n",
    "X_ref_even = jnp.linspace(*bounds, *num_ref_points)\n",
    "\n",
    "model_rand = SparseGPR(X_ref_rand.reshape(-1,1), kernel, init_kernel_params, noise, logger=None)\n",
    "model_even = SparseGPR(X_ref_even.reshape(-1,1), kernel, init_kernel_params, noise, logger=None)\n",
    "model_optim = SparseGPR(X_ref_rand.reshape(-1,1), kernel, init_kernel_params, noise, optimize_noise=True, optimize_ref=True, logger=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rand.train(X_split, Y_train)\n",
    "model_even.train(X_split, Y_train)\n",
    "model_optim.train(X_split, Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with the trained model\n",
    "\n",
    "We will again make predictions on an evenly spaced grid of 200 points in the above defined boundaries for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_grid = jnp.linspace(*bounds, 200)\n",
    "\n",
    "# model.eval returns a mean prediction and a confidence interval around the mean prediction\n",
    "means_rand, stds_rand = model_rand.eval(predict_grid.reshape(-1,1))\n",
    "means_even, stds_even = model_even.eval(predict_grid.reshape(-1,1))\n",
    "means_optim, stds_optim = model_optim.eval(predict_grid.reshape(-1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the result\n",
    "\n",
    "Plotting the predicted function together with its confidence interval we can see that even with only a few datapoints the prediction works good. Close to the reference points the model works similar to the full GPR, however, far away the uncertainty get larger. The shaded region describes the $1\\sigma$ confidence interval.\n",
    "\n",
    "Comparing both methods of choosing a reference grid, the even grid wins out in predictive power. However, one has to be careful to not draw conclusions from such a simple function. In general it is worth testing both methods to see what works best for the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(15,4), sharey=True)\n",
    "\n",
    "ax[0].plot(predict_grid, means_rand, label=\"prediction\")\n",
    "ax[0].fill_between(predict_grid, means_rand-stds_rand, means_rand+stds_rand, alpha=0.5)\n",
    "\n",
    "ax[1].plot(predict_grid, means_even, label=\"prediction\")\n",
    "ax[1].fill_between(predict_grid, means_even-stds_even, means_even+stds_even, alpha=0.5)\n",
    "\n",
    "ax[2].plot(predict_grid, means_optim, label=\"prediction\")\n",
    "ax[2].fill_between(predict_grid, means_optim-stds_optim, means_optim+stds_optim, alpha=0.5)\n",
    "\n",
    "ax[0].plot(predict_grid, sin(predict_grid), c=\"gray\", ls=\"--\",label=\"true function\")\n",
    "ax[1].plot(predict_grid, sin(predict_grid), c=\"gray\", ls=\"--\",label=\"true function\")\n",
    "ax[2].plot(predict_grid, sin(predict_grid), c=\"gray\", ls=\"--\",label=\"true function\")\n",
    "\n",
    "ax[0].scatter(x_func, y_func, c=\"r\", label=\"function eval\")\n",
    "ax[1].scatter(x_func, y_func, c=\"r\", label=\"function eval\")\n",
    "ax[2].scatter(x_func, y_func, c=\"r\", label=\"function eval\")\n",
    "for i,x in enumerate(x_der): \n",
    "    if i == 0:\n",
    "        ax[0].axvline(x, c=\"r\", lw=0.8, ls=\"-\", label=\"deriv positions\")\n",
    "        ax[1].axvline(x, c=\"r\", lw=0.8, ls=\"-\", label=\"deriv positions\")\n",
    "        ax[2].axvline(x, c=\"r\", lw=0.8, ls=\"-\", label=\"deriv positions\")\n",
    "    else:\n",
    "        ax[0].axvline(x, c=\"r\", lw=0.8, ls=\"-\")\n",
    "        ax[1].axvline(x, c=\"r\", lw=0.8, ls=\"-\")\n",
    "        ax[2].axvline(x, c=\"r\", lw=0.8, ls=\"-\")\n",
    "\n",
    "for i,x in enumerate(X_ref_rand):\n",
    "    if i == 0:\n",
    "        ax[0].axvline(x, c=\"k\", lw=0.8, ls=\"--\", label=\"reference positions\")\n",
    "    else:\n",
    "        ax[0].axvline(x, c=\"k\", lw=0.8, ls=\"--\")\n",
    "        \n",
    "for i,x in enumerate(X_ref_even):\n",
    "    if i == 0:\n",
    "        ax[1].axvline(x, c=\"k\", lw=0.8, ls=\"--\", label=\"reference positions\")\n",
    "    else:\n",
    "        ax[1].axvline(x, c=\"k\", lw=0.8, ls=\"--\")\n",
    "        \n",
    "for i,x in enumerate(model_optim.X_ref):\n",
    "    if i == 0:\n",
    "        ax[2].axvline(x, c=\"k\", lw=0.8, ls=\"--\", label=\"reference positions\")\n",
    "    else:\n",
    "        ax[2].axvline(x, c=\"k\", lw=0.8, ls=\"--\")\n",
    "\n",
    "\n",
    "ax[0].set_title(\"Random\")\n",
    "ax[1].set_title(\"Even\")\n",
    "ax[2].set_title(\"Optimized\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "ax[2].grid()\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing full and sparse GPR to a standard numerical integration via the trapezoidal rule\n",
    "\n",
    "Now we repeat the datageneration as above on an evenly spaced grid.\n",
    "\n",
    "The reference grid is chosen to be evenly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data points at which to sample the function and its derivative\n",
    "x_der = jnp.linspace(*bounds, *num_d_vals)\n",
    "x_func = jnp.linspace(*bounds, *num_f_vals)\n",
    "\n",
    "#create training data\n",
    "X_split = [x_func.reshape(-1,1),x_der.reshape(-1,1)]\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "y_func = sin(x_func,noise, subkey)\n",
    "key, subkey = random.split(key)\n",
    "y_der = cos(x_der, noise, subkey)\n",
    "\n",
    "Y_train = jnp.hstack((y_func, y_der))\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "X_ref = random.choice(subkey, x_der, num_ref_points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also train the model again and predict values on the same grid as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExactGPR(kernel, init_kernel_params, noise)\n",
    "model.train(X_split, Y_train)\n",
    "\n",
    "means_full, stds_full = model.eval(predict_grid.reshape(-1,1))\n",
    "\n",
    "model = SparseGPR(X_ref.reshape(-1,1), kernel, init_kernel_params, noise, optimize_noise=True, optimize_ref=True)\n",
    "model.train(X_split, Y_train)\n",
    "\n",
    "means_sparse, stds_sparse = model.eval(predict_grid.reshape(-1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the trapezoidal rule to numerically integrate the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trapsum = []\n",
    "for i,elem in enumerate(y_der):\n",
    "    trapsum.append(jnp.sum(y_der[:i+1]) - 0.5*(y_der[0] + y_der[i]))\n",
    "trapsum = jnp.array(trapsum) * (x_der[1] - x_der[0])\n",
    "\n",
    "# Add the function value at zero to fix the integration constant\n",
    "trapsum += y_func"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result for comparison. The shaded regions describe the $1\\sigma$ confidence intervals around the mean prediction.\n",
    "\n",
    "While the accuracy is comparable, the GPR produces a smooth fit and also gives a confidence interval around the prediction.\n",
    "\n",
    "Comparing also the full GPR with the sparse GPR the fitted functions are similar, however, for the sparse version the confidence intervals are larger in between the reference points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predict_grid, means_full, label=\"full prediction\")\n",
    "plt.fill_between(predict_grid, means_full-stds_full, means_full+stds_full, alpha=0.5)\n",
    "\n",
    "plt.plot(predict_grid, means_sparse, label=\"sparse prediction\")\n",
    "plt.fill_between(predict_grid, means_sparse-stds_sparse, means_sparse+stds_sparse, alpha=0.5)\n",
    "\n",
    "plt.plot(x_der,trapsum, label=\"trapezoid\")\n",
    "\n",
    "plt.plot(predict_grid, sin(predict_grid), c=\"gray\", ls=\"--\",label=\"true function\")\n",
    "\n",
    "plt.scatter(x_func, y_func, c=\"r\", label=\"function eval\")\n",
    "for i,x in enumerate(x_der): \n",
    "    if i == 0:\n",
    "        plt.axvline(x, c=\"r\", lw=0.8, ls=\"-\", label=\"deriv positions\")\n",
    "    else:\n",
    "        plt.axvline(x, c=\"r\", lw=0.8, ls=\"-\")\n",
    "\n",
    "for i,x in enumerate(model.X_ref):\n",
    "    if i == 0:\n",
    "        plt.axvline(x, c=\"k\", lw=0.8, ls=\"--\", label=\"reference positions\")\n",
    "    else:\n",
    "        plt.axvline(x, c=\"k\", lw=0.8, ls=\"--\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
