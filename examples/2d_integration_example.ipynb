{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recovering a 2d function from its gradient with a Gaussian Process Regression model\n",
    "\n",
    "This notebook shows examples on how to use the present gaussian process regression framework to formally integrate functions from derivative observations.\n",
    "\n",
    "This example will show how to predict a 2D function using gradient observations with both a full GPR and a sparse GPR. The sparse GPR framework works by projecting the training data into a lower dimensional feature space in order to reduce the computational cost (mainly matrix inversions)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training data set\n",
    "\n",
    "`jax.numpy` has almost the same usage as the standard `numpy` package, with the caveat that `jax.ndarray` is an immutable type, meaning that no inplace changes can be made. For creating training data this should however not be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will model a forth order polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x, noise=0.0, key = random.PRNGKey(0)):\n",
    "    return (x[:,0]**2 + x[:,1] - 11)**2 / 800.0 + (x[:,0] + x[:,1]**2 -7)**2 / 800.0 + random.normal(key,(len(x),), dtype=jnp.float32)*noise\n",
    "\n",
    "def grad(x, noise=0.0, key = random.PRNGKey(0)):\n",
    "    dx1 = 4 * (x[:,0]**2 + x[:,1] - 11) * x[:,0] + 2 * (x[:,0] + x[:,1]**2 -7)\n",
    "    dx2 = 2 * (x[:,0]**2 + x[:,1] - 11) + 4 * (x[:,0] + x[:,1]**2 -7) * x[:,1]\n",
    "\n",
    "    return jnp.vstack((dx1, dx2)).T / 800.0 + random.normal(key,x.shape, dtype=jnp.float32)*noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the training data we first need to define boundaries to choose the datapoints from. Then, random points are chosen in this interval. `random.split` creates a new subkey from the previous key to get a new sudo-random sample from the uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interval bounds from which to choose the data points\n",
    "bounds = jnp.array([-5.0, 5.0])\n",
    "\n",
    "# How many function and derivative observations should be chosen\n",
    "num_f_vals = 1\n",
    "num_d_vals = 100\n",
    "\n",
    "# initial seed for the pseudo random key generation\n",
    "seed = 3\n",
    "\n",
    "# create new keys and randomly sample the above interval for training features\n",
    "key, subkey = random.split(random.PRNGKey(seed))\n",
    "x_func = random.uniform(subkey, (num_f_vals, 2), minval=bounds[0], maxval=bounds[1])\n",
    "key, subkey = random.split(key)\n",
    "x_der = random.uniform(subkey, (num_d_vals,2), minval=bounds[0], maxval=bounds[1])\n",
    "\n",
    "# noise with which to sample the training labels\n",
    "noise = 0.02\n",
    "key, subkey = random.split(key)\n",
    "y_func = fun(x_func,noise, subkey)\n",
    "key, subkey = random.split(key)\n",
    "y_der = grad(x_der, noise, subkey)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPR framework needs as input for training a tuple of arrays `X_split` of which contains a set of points where the function is sampled and a set of points where the gradient is sampled. Both array in `X_split` is of shape `(n_samples_i, N)`. `X_split` should be ordered as follows: the first array represents the datapoints for the function observations and the second array represents the gradient of the function. `Y_train` should just be an array of shape `(n_samples_function + n_samples_gradient,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_split = [x_func,x_der]\n",
    "\n",
    "Y_train = jnp.hstack((y_func, y_der.reshape(-1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Kernel and its initial parameters\n",
    "\n",
    "The kernels can be found in `jaxgp.kernels`. Currently implemented are `RBF`, `Linear`, and `Periodic` kernels. When in doubt what kernel to use, go with an `RBF` kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.kernels import RBF\n",
    "\n",
    "kernel = RBF()\n",
    "# an RBF kernel has per default 2 parameters\n",
    "init_kernel_params = jnp.ones(2)*jnp.log(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The full GPR model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the full GPR model\n",
    "\n",
    "The regression models can be found in `jaxgp.regression`. The `ExactGPR` model takes as inital arguments a kernel to use, initial parameters for the kernel and a noise parameter that underlies the noisy datageneration.\n",
    "\n",
    "Additionally one can also give the model a logging function that saves the parameters at each step of the optimization. For this a convenience class `Logger` can be found in `jaxgp.utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.regression import ExactGPR\n",
    "from jaxgp.utils import Logger\n",
    "\n",
    "# logger = Logger()\n",
    "model = ExactGPR(kernel, init_kernel_params, noise, optimize_noise=True) #, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(X_split, Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterations of the parameters during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(logger.iters_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with the trained model\n",
    "\n",
    "We will make predictions on an evenly spaced grid of $101\\times101$ points square box defined by the above bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_grid = jnp.array(jnp.meshgrid(jnp.linspace(*bounds, 101), jnp.linspace(*bounds, 101))).T.reshape(101**2, 2)\n",
    "\n",
    "# model.eval returns a mean prediction and a confidence interval around the mean prediction\n",
    "means, stds = model.eval(predict_grid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the result\n",
    "\n",
    "Since the system is now 2D we squared the number of function and gradient observations. Comparing the contour plot of the prediction with the true function, the results are quite passable. However, in the 2D case it looks like there is a need for more function evaluations. Maybe also at some strategic points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "means = means.reshape(101,101)\n",
    "stds = stds.reshape(101,101)\n",
    "true = fun(predict_grid).reshape(101,101)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10,7))\n",
    "\n",
    "mesh = jnp.meshgrid(jnp.linspace(*bounds, 101),jnp.linspace(*bounds, 101))\n",
    "\n",
    "im1 = ax[0,0].contourf(*mesh, means, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im2 = ax[0,1].contourf(*mesh, true, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im3 = ax[1,0].contourf(*mesh, stds)\n",
    "im4 = ax[1,1].contourf(*mesh, jnp.abs(means-true))\n",
    "# im4 = ax[1,1].contourf(*mesh, jnp.greater(true, means-stds)*jnp.less(true, means+stds))\n",
    "\n",
    "ax[1,1].scatter(x_der[:,0], x_der[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "ax[1,1].scatter(x_func[:,0], x_func[:,1], c=\"orange\", marker=\"+\", label=\"fun pos\")\n",
    "\n",
    "ax[1,0].scatter(x_der[:,0], x_der[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "ax[1,0].scatter(x_func[:,0], x_func[:,1], c=\"orange\", marker=\"+\", label=\"fun pos\")\n",
    "\n",
    "plt.colorbar(im1, ax=ax[0,0])\n",
    "plt.colorbar(im2, ax=ax[0,1])\n",
    "plt.colorbar(im3, ax=ax[1,0])\n",
    "plt.colorbar(im4, ax=ax[1,1])\n",
    "\n",
    "ax[0,0].set_title(\"prediction\")\n",
    "ax[0,1].set_title(\"true function\")\n",
    "ax[1,0].set_title(\"std\")\n",
    "ax[1,1].set_title(\"abs dif\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sparse GPR model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the sparse GPR model\n",
    "\n",
    "The `sparseGPR` model can be found in `jaxgp.regression`. The idea of a sparse model is to project your training data into a space with smaller dimension in order to save in computational cost. This is done by projecting your full training set onto a set of reference points via the kernel. \n",
    "\n",
    "There are typically 3 methods to choose reference points:\n",
    " - choosing a subset of size $m<n$ from the existing datapoints\n",
    " - creating an even grid inside the bounds on which the model should be evaluated\n",
    " - Choosing a fixed number of reference points and then optimizing their locations\n",
    "\n",
    "Note that below the evenly spread reference grid has fewer points than the subset grid. This was done in order to get a nice even grid which was achieved by taking the largerst perfect square smaller than the number of reference points.\n",
    "\n",
    "Furthermore we also created a larger set of gradient observations since the predictive power of the sparse model is lower than of the full model. However, this is not a problem computation wise as is seen further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.regression import SparseGPR\n",
    "\n",
    "# How many function and derivative observations should be chosen\n",
    "num_f_vals = 1\n",
    "num_d_vals = 1000\n",
    "\n",
    "# initial seed for the pseudo random key generation\n",
    "seed = 0\n",
    "\n",
    "# create new keys and randomly sample the above interval for training features\n",
    "key, subkey = random.split(random.PRNGKey(seed))\n",
    "x_func = random.uniform(subkey, (num_f_vals, 2), minval=bounds[0], maxval=bounds[1])\n",
    "key, subkey = random.split(key)\n",
    "x_der = random.uniform(subkey, (num_d_vals,2), minval=bounds[0], maxval=bounds[1])\n",
    "\n",
    "# noise with which to sample the training labels\n",
    "noise = 0.02\n",
    "key, subkey = random.split(key)\n",
    "y_func = fun(x_func,noise, subkey)\n",
    "key, subkey = random.split(key)\n",
    "y_der = grad(x_der, noise, subkey)\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "num_ref_points = (num_d_vals + num_f_vals) // 10\n",
    "key, subkey = random.split(key)\n",
    "X_ref_rand = random.permutation(subkey, jnp.vstack((x_der,x_func)))[:num_ref_points]\n",
    "# this grid has fewer points. It has N points, where N is the largest perfect square smaller than num_ref_points\n",
    "X_ref_even = jnp.array(jnp.meshgrid(jnp.linspace(*bounds, round(jnp.sqrt(num_ref_points))),jnp.linspace(*bounds, round(jnp.sqrt(num_ref_points))))).reshape(2,-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_noise = True\n",
    "optimize_ref = True\n",
    "\n",
    "model_rand = SparseGPR(kernel, init_kernel_params, noise, X_ref_rand, logger=None)\n",
    "model_even = SparseGPR(kernel, init_kernel_params, noise, X_ref_even, logger=None)\n",
    "model_optim = SparseGPR(kernel, init_kernel_params, noise, X_ref_rand, optimize_noise=optimize_noise, optimize_ref=optimize_ref, logger=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_split = [x_func,x_der]\n",
    "\n",
    "Y_train = jnp.hstack((y_func, y_der.reshape(-1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the time needed to train the sparse models, even with 10 times as many datapoints the computation is still twice as fast, compare to the full model. This is because the computational effort is roughly 100 times smaller: \n",
    " - The full model needs $\\mathcal{O}(N^3)$ flops to train and fit the model\n",
    " - The sparse model needs $\\mathcal{O}(M^2N + M^3)$ flops to train and fit the model\n",
    "\n",
    "which comes out to roughly a factor 100 times faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rand.train(X_split, Y_train)\n",
    "model_even.train(X_split, Y_train)\n",
    "model_optim.train(X_split, Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with the trained model\n",
    "\n",
    "We will make predictions on an evenly spaced grid of $101\\times101$ points square box defined by the above bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_grid = jnp.array(jnp.meshgrid(jnp.linspace(*bounds, 101), jnp.linspace(*bounds, 101))).T.reshape(101**2, 2)\n",
    "\n",
    "# model.eval returns a mean prediction and a confidence interval around the mean prediction\n",
    "means_rand, stds_rand = model_rand.eval(predict_grid)\n",
    "means_even, stds_even = model_even.eval(predict_grid)\n",
    "means_optim, stds_optim = model_optim.eval(predict_grid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the result\n",
    "\n",
    "#### Random subset reference points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "means = means_rand.reshape(101,101)\n",
    "stds = stds_rand.reshape(101,101)\n",
    "true = fun(predict_grid).reshape(101,101)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10,7))\n",
    "\n",
    "mesh = jnp.meshgrid(jnp.linspace(*bounds, 101),jnp.linspace(*bounds, 101))\n",
    "\n",
    "im1 = ax[0,0].contourf(*mesh, means, levels=12)#, vmin=-0.15, vmax=1.2)\n",
    "im2 = ax[0,1].contourf(*mesh, true, levels=12)#, vmin=-0.15, vmax=1.2)\n",
    "im3 = ax[1,0].contourf(*mesh, stds)\n",
    "im4 = ax[1,1].contourf(*mesh, jnp.abs(means-true))\n",
    "\n",
    "plt.colorbar(im1, ax=ax[0,0])\n",
    "plt.colorbar(im2, ax=ax[0,1])\n",
    "plt.colorbar(im3, ax=ax[1,0])\n",
    "plt.colorbar(im4, ax=ax[1,1])\n",
    "\n",
    "ax[1,1].scatter(X_ref_rand[:,0], X_ref_rand[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "\n",
    "ax[1,0].scatter(X_ref_rand[:,0], X_ref_rand[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "\n",
    "ax[0,0].set_title(\"prediction\")\n",
    "ax[0,1].set_title(\"true function\")\n",
    "ax[1,0].set_title(\"std\")\n",
    "ax[1,1].set_title(\"abs dif\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evenly spaced reference points\n",
    "\n",
    "Note that both produce similar results in this case. However, the evenly spaced grid lacks some stability and for different configurations the resulting standard deviations can't be calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = means_even.reshape(101,101)\n",
    "stds = stds_even.reshape(101,101)\n",
    "true = fun(predict_grid).reshape(101,101)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10,7))\n",
    "\n",
    "mesh = jnp.meshgrid(jnp.linspace(*bounds, 101),jnp.linspace(*bounds, 101))\n",
    "\n",
    "im1 = ax[0,0].contourf(*mesh, means, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im2 = ax[0,1].contourf(*mesh, true, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im3 = ax[1,0].contourf(*mesh, stds)\n",
    "im4 = ax[1,1].contourf(*mesh, jnp.abs(means-true))\n",
    "\n",
    "plt.colorbar(im1, ax=ax[0,0])\n",
    "plt.colorbar(im2, ax=ax[0,1])\n",
    "plt.colorbar(im3, ax=ax[1,0])\n",
    "plt.colorbar(im4, ax=ax[1,1])\n",
    "\n",
    "ax[1,1].scatter(X_ref_even[:,0], X_ref_even[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "\n",
    "ax[1,0].scatter(X_ref_even[:,0], X_ref_even[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "\n",
    "ax[0,0].set_title(\"prediction\")\n",
    "ax[0,1].set_title(\"true function\")\n",
    "ax[1,0].set_title(\"std\")\n",
    "ax[1,1].set_title(\"abs dif\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimized grid\n",
    "\n",
    "Note that some reference points are located outside of the region where the function was defined. However, this should not be a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = means_optim.reshape(101,101)\n",
    "stds = stds_optim.reshape(101,101)\n",
    "true = fun(predict_grid).reshape(101,101)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10,7))\n",
    "\n",
    "mesh = jnp.meshgrid(jnp.linspace(*bounds, 101),jnp.linspace(*bounds, 101))\n",
    "\n",
    "im1 = ax[0,0].contourf(*mesh, means, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im2 = ax[0,1].contourf(*mesh, true, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im3 = ax[1,0].contourf(*mesh, stds)\n",
    "im4 = ax[1,1].contourf(*mesh, jnp.abs(means-true))\n",
    "\n",
    "plt.colorbar(im1, ax=ax[0,0])\n",
    "plt.colorbar(im2, ax=ax[0,1])\n",
    "plt.colorbar(im3, ax=ax[1,0])\n",
    "plt.colorbar(im4, ax=ax[1,1])\n",
    "\n",
    "ax[1,1].scatter(model_optim.X_ref[:,0], model_optim.X_ref[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "\n",
    "ax[1,0].scatter(model_optim.X_ref[:,0], model_optim.X_ref[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "\n",
    "ax[0,0].set_title(\"prediction\")\n",
    "ax[0,1].set_title(\"true function\")\n",
    "ax[1,0].set_title(\"std\")\n",
    "ax[1,1].set_title(\"abs dif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
