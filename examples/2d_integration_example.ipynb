{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recovering a 2d function from its gradient with a Gaussian Process Regression model\n",
    "\n",
    "This notebook shows examples on how to use the present gaussian process regression framework to formally integrate functions from derivative observations.\n",
    "\n",
    "This example will show how to predict a 2D function using gradient observations with both a full GPR and a sparse GPR. The sparse GPR framework works by projecting the training data onto a lower dimensional reference set in order to reduce the computational cost (mainly matrix inversions)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training data set\n",
    "\n",
    "`jax.numpy` has almost the same usage as the standard `numpy` package, with the caveat that `jax.ndarray` is an immutable type, meaning that no inplace changes can be made. For creating training data this should however not be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will model a forth order polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x, noise=0.0, key = random.PRNGKey(0)):\n",
    "    return (x[:,0]**2 + x[:,1] - 11)**2 / 800.0 + (x[:,0] + x[:,1]**2 -7)**2 / 800.0 + random.normal(key,(len(x),), dtype=jnp.float32)*noise\n",
    "\n",
    "def grad(x, noise=0.0, key = random.PRNGKey(0)):\n",
    "    dx1 = 4 * (x[:,0]**2 + x[:,1] - 11) * x[:,0] + 2 * (x[:,0] + x[:,1]**2 -7)\n",
    "    dx2 = 2 * (x[:,0]**2 + x[:,1] - 11) + 4 * (x[:,0] + x[:,1]**2 -7) * x[:,1]\n",
    "\n",
    "    return jnp.vstack((dx1, dx2)).T / 800.0 + random.normal(key,x.shape, dtype=jnp.float32)*noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the training data we first need to define boundaries to choose the datapoints from. Then, random points are chosen in this interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interval bounds from which to choose the data points\n",
    "bounds = jnp.array([-5.0, 5.0])\n",
    "\n",
    "# How many function and derivative observations should be chosen\n",
    "num_f_vals = 1\n",
    "num_d_vals = 100\n",
    "\n",
    "# initial seed for the pseudo random key generation\n",
    "seed = 3\n",
    "\n",
    "# create new keys and randomly sample the above interval for training features\n",
    "key, subkey = random.split(random.PRNGKey(seed))\n",
    "x_func = random.uniform(subkey, (num_f_vals, 2), minval=bounds[0], maxval=bounds[1])\n",
    "key, subkey = random.split(key)\n",
    "x_der = random.uniform(subkey, (num_d_vals,2), minval=bounds[0], maxval=bounds[1])\n",
    "\n",
    "# noise with which to sample the training labels\n",
    "noise = 0.02\n",
    "key, subkey = random.split(key)\n",
    "y_func = fun(x_func,noise, subkey)\n",
    "key, subkey = random.split(key)\n",
    "y_der = grad(x_der, noise, subkey)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPR framework needs as input for training a tuple of arrays `X_split`. It contains two arrays `(X_f, X_g)` which describe the input points at which the function and the gradient was observed.\n",
    "\n",
    "`Y_train` should just be an array of shape `(n_samples_function + n_samples_gradient,)` and it holds the noisy function and gradient oberservations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_split = [x_func,x_der]\n",
    "\n",
    "Y_train = jnp.hstack((y_func, y_der.reshape(-1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Kernel\n",
    "\n",
    "Kernels describe the covariance between two input points. They are used to relate how much a function observation at point `x` will effect the prediction at another point `x_*`.\n",
    "\n",
    "The kernels can be found in `jaxgp.kernels`. Currently implemented are `RBF`, `Linear`, and `Periodic` kernels. In general the `RBF` kernel works in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.kernels import RBF\n",
    "\n",
    "kernel = RBF()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The full GPR model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the full GPR model\n",
    "\n",
    "The regression models can be found in `jaxgp.regression`. The `ExactGPR` model takes as inital arguments a kernel to use and a noise parameter that underlies the noisy datageneration.\n",
    "\n",
    "Additionally one can also give the model a logging function that saves the parameters at each step of the optimization. For this a convenience class `Logger` can be found in `jaxgp.utils`.\n",
    "\n",
    "It is also possible to provide custom inital parameters for the kernel. However, this is in most cases unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.regression import ExactGPR\n",
    "from jaxgp.utils import Logger\n",
    "\n",
    "logger = Logger()\n",
    "model = ExactGPR(kernel, noise=noise, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(X_split, Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterations of the parameters during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, elem in enumerate(logger.iters_list):\n",
    "    # print(f\"kernel_params: {elem[0]}, noise: {elem[1]:.3f}\")\n",
    "    print(f\"iter {i}: {elem}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with the trained model\n",
    "\n",
    "We will make predictions on an evenly spaced grid of $101\\times101$ points square box defined by the above bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_grid = jnp.array(jnp.meshgrid(jnp.linspace(*bounds, 101), jnp.linspace(*bounds, 101))).T.reshape(101**2, 2)\n",
    "\n",
    "# model.eval returns a mean prediction and a confidence interval around the mean prediction\n",
    "means, stds = model.eval(predict_grid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the result\n",
    "\n",
    "Since the system is now 2D we squared the number of function and gradient observations. Comparing the contour plot of the prediction with the true function, the results are prettz good. However, in the 2D case it looks like there is a need for more function evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "means = means.reshape(101,101)\n",
    "stds = stds.reshape(101,101)\n",
    "true = fun(predict_grid).reshape(101,101)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10,7))\n",
    "\n",
    "mesh = jnp.meshgrid(jnp.linspace(*bounds, 101),jnp.linspace(*bounds, 101))\n",
    "\n",
    "im1 = ax[0,0].contourf(*mesh, means, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im2 = ax[0,1].contourf(*mesh, true, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im3 = ax[1,0].contourf(*mesh, stds)\n",
    "im4 = ax[1,1].contourf(*mesh, jnp.abs(means-true))\n",
    "# im4 = ax[1,1].contourf(*mesh, jnp.greater(true, means-stds)*jnp.less(true, means+stds))\n",
    "\n",
    "ax[1,1].scatter(x_der[:,0], x_der[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "ax[1,1].scatter(x_func[:,0], x_func[:,1], c=\"orange\", marker=\"+\", label=\"fun pos\")\n",
    "\n",
    "ax[1,0].scatter(x_der[:,0], x_der[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "ax[1,0].scatter(x_func[:,0], x_func[:,1], c=\"orange\", marker=\"+\", label=\"fun pos\")\n",
    "\n",
    "plt.colorbar(im1, ax=ax[0,0])\n",
    "plt.colorbar(im2, ax=ax[0,1])\n",
    "plt.colorbar(im3, ax=ax[1,0])\n",
    "plt.colorbar(im4, ax=ax[1,1])\n",
    "\n",
    "ax[0,0].set_title(\"prediction\")\n",
    "ax[0,1].set_title(\"true function\")\n",
    "ax[1,0].set_title(\"std\")\n",
    "ax[1,1].set_title(\"abs dif\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sparse GPR model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the sparse GPR model\n",
    "\n",
    "The `sparseGPR` model can be found in `jaxgp.regression`. The idea of a sparse model is to project your training data into a space with smaller dimension in order to save in computational cost. This is done by projecting your full training set onto a set of reference points via the kernel. \n",
    "\n",
    "There are typically 3 methods to choose reference points:\n",
    " - choosing a subset of size $m<n$ from the existing datapoints\n",
    " - creating an even grid inside the bounds on which the model should be evaluated\n",
    " - Choosing a fixed number of reference points and then optimizing their locations\n",
    "\n",
    "Note that below the evenly spread reference grid has fewer points than the subset grid. This was done in order to get a nice even grid which was achieved by taking the largerst perfect square smaller than the number of reference points.\n",
    "\n",
    "Furthermore we also created a larger set of gradient observations since the predictive power of the sparse model is lower than of the full model. However, this is not a problem computation wise as is seen further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.regression import SparseGPR\n",
    "\n",
    "# How many function and derivative observations should be chosen\n",
    "num_f_vals = 1\n",
    "num_d_vals = 1000\n",
    "\n",
    "# initial seed for the pseudo random key generation\n",
    "seed = 0\n",
    "\n",
    "# create new keys and randomly sample the above interval for training features\n",
    "key, subkey = random.split(random.PRNGKey(seed))\n",
    "x_func = random.uniform(subkey, (num_f_vals, 2), minval=bounds[0], maxval=bounds[1])\n",
    "key, subkey = random.split(key)\n",
    "x_der = random.uniform(subkey, (num_d_vals,2), minval=bounds[0], maxval=bounds[1])\n",
    "\n",
    "# noise with which to sample the training labels\n",
    "noise = 0.02\n",
    "key, subkey = random.split(key)\n",
    "y_func = fun(x_func,noise, subkey)\n",
    "key, subkey = random.split(key)\n",
    "y_der = grad(x_der, noise, subkey)\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "num_ref_points = (num_d_vals + num_f_vals) // 20\n",
    "key, subkey = random.split(key)\n",
    "X_ref_rand = random.permutation(subkey, jnp.vstack((x_der,x_func)))[:num_ref_points]\n",
    "# this grid has fewer points. It has N points, where N is the largest perfect square smaller than num_ref_points\n",
    "X_ref_even = jnp.array(jnp.meshgrid(jnp.linspace(*bounds, round(jnp.sqrt(num_ref_points))),jnp.linspace(*bounds, round(jnp.sqrt(num_ref_points))))).reshape(2,-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_noise = True\n",
    "optimize_ref = True\n",
    "\n",
    "model_rand = SparseGPR(X_ref_rand, kernel, noise=noise, logger=None)\n",
    "model_even = SparseGPR(X_ref_even, kernel, noise=noise, logger=None)\n",
    "model_optim = SparseGPR(X_ref_rand, kernel, noise=noise, optimize_noise=optimize_noise, optimize_ref=optimize_ref, ref_bounds=(-5.0,5.0),logger=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_split = [x_func,x_der]\n",
    "\n",
    "Y_train = jnp.hstack((y_func, y_der.reshape(-1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the time needed to train the sparse models, even with 10 times as many datapoints the computation is still twice as fast, compare to the full model. This is because the computational effort is roughly 100 times smaller: \n",
    " - The full model needs $\\mathcal{O}(N^3)$ flops to train and fit the model\n",
    " - The sparse model needs $\\mathcal{O}(M^2N)$ flops to train and fit the model\n",
    "\n",
    "which comes out to roughly a factor 100 times faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rand.train(X_split, Y_train)\n",
    "model_even.train(X_split, Y_train)\n",
    "model_optim.train(X_split, Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with the trained model\n",
    "\n",
    "We will make predictions on an evenly spaced grid of $101\\times101$ points square box defined by the above bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_grid = jnp.array(jnp.meshgrid(jnp.linspace(*bounds, 101), jnp.linspace(*bounds, 101))).T.reshape(101**2, 2)\n",
    "\n",
    "# model.eval returns a mean prediction and a confidence interval around the mean prediction\n",
    "means_rand, stds_rand = model_rand.eval(predict_grid)\n",
    "means_even, stds_even = model_even.eval(predict_grid)\n",
    "means_optim, stds_optim = model_optim.eval(predict_grid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the result\n",
    "\n",
    "#### Random subset reference points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "means = means_rand.reshape(101,101)\n",
    "stds = stds_rand.reshape(101,101)\n",
    "true = fun(predict_grid).reshape(101,101)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10,7))\n",
    "\n",
    "mesh = jnp.meshgrid(jnp.linspace(*bounds, 101),jnp.linspace(*bounds, 101))\n",
    "\n",
    "im1 = ax[0,0].contourf(*mesh, means, levels=12)#, vmin=-0.15, vmax=1.2)\n",
    "im2 = ax[0,1].contourf(*mesh, true, levels=12)#, vmin=-0.15, vmax=1.2)\n",
    "im3 = ax[1,0].contourf(*mesh, stds)\n",
    "im4 = ax[1,1].contourf(*mesh, jnp.abs(means-true))\n",
    "\n",
    "plt.colorbar(im1, ax=ax[0,0])\n",
    "plt.colorbar(im2, ax=ax[0,1])\n",
    "plt.colorbar(im3, ax=ax[1,0])\n",
    "plt.colorbar(im4, ax=ax[1,1])\n",
    "\n",
    "ax[1,1].scatter(X_ref_rand[:,0], X_ref_rand[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "\n",
    "ax[1,0].scatter(X_ref_rand[:,0], X_ref_rand[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "\n",
    "ax[0,0].set_title(\"prediction\")\n",
    "ax[0,1].set_title(\"true function\")\n",
    "ax[1,0].set_title(\"std\")\n",
    "ax[1,1].set_title(\"abs dif\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evenly spaced reference points\n",
    "\n",
    "Note that both produce similar results in this case. However, the evenly spaced grid lacks some stability and for different configurations the resulting standard deviations can't be calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = means_even.reshape(101,101)\n",
    "stds = stds_even.reshape(101,101)\n",
    "true = fun(predict_grid).reshape(101,101)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10,7))\n",
    "\n",
    "mesh = jnp.meshgrid(jnp.linspace(*bounds, 101),jnp.linspace(*bounds, 101))\n",
    "\n",
    "im1 = ax[0,0].contourf(*mesh, means, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im2 = ax[0,1].contourf(*mesh, true, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im3 = ax[1,0].contourf(*mesh, stds)\n",
    "im4 = ax[1,1].contourf(*mesh, jnp.abs(means-true))\n",
    "\n",
    "plt.colorbar(im1, ax=ax[0,0])\n",
    "plt.colorbar(im2, ax=ax[0,1])\n",
    "plt.colorbar(im3, ax=ax[1,0])\n",
    "plt.colorbar(im4, ax=ax[1,1])\n",
    "\n",
    "ax[1,1].scatter(X_ref_even[:,0], X_ref_even[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "\n",
    "ax[1,0].scatter(X_ref_even[:,0], X_ref_even[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "\n",
    "ax[0,0].set_title(\"prediction\")\n",
    "ax[0,1].set_title(\"true function\")\n",
    "ax[1,0].set_title(\"std\")\n",
    "ax[1,1].set_title(\"abs dif\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimized grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = means_optim.reshape(101,101)\n",
    "stds = stds_optim.reshape(101,101)\n",
    "true = fun(predict_grid).reshape(101,101)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10,7))\n",
    "\n",
    "mesh = jnp.meshgrid(jnp.linspace(*bounds, 101),jnp.linspace(*bounds, 101))\n",
    "\n",
    "im1 = ax[0,0].contourf(*mesh, means, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im2 = ax[0,1].contourf(*mesh, true, levels=12, vmin=-0.15, vmax=1.2)\n",
    "im3 = ax[1,0].contourf(*mesh, stds)\n",
    "im4 = ax[1,1].contourf(*mesh, jnp.abs(means-true))\n",
    "\n",
    "plt.colorbar(im1, ax=ax[0,0])\n",
    "plt.colorbar(im2, ax=ax[0,1])\n",
    "plt.colorbar(im3, ax=ax[1,0])\n",
    "plt.colorbar(im4, ax=ax[1,1])\n",
    "\n",
    "ax[1,1].scatter(model_optim.X_ref[:,0], model_optim.X_ref[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "\n",
    "ax[1,0].scatter(model_optim.X_ref[:,0], model_optim.X_ref[:,1], c=\"r\", marker=\"x\", label=\"der pos\")\n",
    "\n",
    "ax[0,0].set_title(\"prediction\")\n",
    "ax[0,1].set_title(\"true function\")\n",
    "ax[1,0].set_title(\"std\")\n",
    "ax[1,1].set_title(\"abs dif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
