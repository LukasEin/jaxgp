{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formally integrating a 1d function with a Gaussian Process Regression model\n",
    "\n",
    "This notebook shows examples on how to use the present gaussian process regression framework to formally integrate functions from derivative observations.\n",
    "\n",
    "This example will show how to predict a 1D function both with a full GPR and a sparse GPR. The sparse GPR framework works by projecting the training data into a lower dimensional feature space in order to reduce the computational cost (mainly matrix inversions)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training data set\n",
    "\n",
    "`jax.numpy` has almost the same usage as the standard `numpy` package, with the caveat that `jax.ndarray` is an immutable type, meaning that no inplace changes can be made. For creating training data this should however not be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will model a simple sin function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true function is a noisy lennard jones potential\n",
    "def sin(x, noise=0.0, key = random.PRNGKey(0)):\n",
    "    return jnp.sin(x) + random.normal(key,x.shape, dtype=jnp.float32)*noise\n",
    "\n",
    "def cos(x, noise=0.0, key = random.PRNGKey(0)):\n",
    "    return jnp.cos(x) + random.normal(key,x.shape, dtype=jnp.float32)*noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the training data we first need to define boundaries to choose the datapoints from. Then, random points are chosen in this interval. `random.split` creates a new subkey from the previous key to get a new sudo-random sample from the uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interval bounds from which to choose the data points\n",
    "bounds = jnp.array([0.0, 2*jnp.pi])\n",
    "\n",
    "# How many function and derivative observations should be chosen\n",
    "num_f_vals = (1,)\n",
    "num_d_vals = (10,)\n",
    "\n",
    "# initial seed for the pseudo random key generation\n",
    "seed = 0\n",
    "\n",
    "# create new keys and randomly sample the above interval for training features\n",
    "key, subkey = random.split(random.PRNGKey(seed))\n",
    "x_func = random.uniform(subkey, num_f_vals, minval=bounds[0], maxval=bounds[1])\n",
    "key, subkey = random.split(key)\n",
    "x_der = random.uniform(subkey, num_d_vals, minval=bounds[0], maxval=bounds[1])\n",
    "\n",
    "# noise with which to sample the training labels\n",
    "noise = 0.1\n",
    "key, subkey = random.split(key)\n",
    "y_func = sin(x_func,noise, subkey)\n",
    "key, subkey = random.split(key)\n",
    "y_der = cos(x_der, noise, subkey)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPR framework needs as input for training a list of arrays `X_split` of length $N + 1$ where $N$ is the dimension of the input feature. Each array in `X_split` is of shape `(n_samples_i, N)`. `X_split` should be ordered as follows: the first array represents the datapoints for the function observations and the $N$ following arrays each represent the partial derivatives of the function with respect to each of the inputs in order. `Y_train` should just be an array of shape `(sum(n_samples_i),)`.\n",
    "\n",
    "In our case $N=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping needs to be done the get the arrays in the form (n_samples_i, N)\n",
    "X_split = [x_func.reshape(-1,1),x_der.reshape(-1,1)]\n",
    "\n",
    "Y_train = jnp.hstack((y_func, y_der))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Kernel and its initial parameters\n",
    "\n",
    "The kernels can be found in `jaxgp.kernels`. Currently implemented are `RBF`, `Linear`, and `Periodic` kernels. When in doubt what kernel to use, go with an `RBF` kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.kernels import RBF\n",
    "\n",
    "kernel = RBF()\n",
    "# an RBF kernel has per default 2 parameters\n",
    "init_kernel_params = (2.0, 2.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The full GPR model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the full GPR model\n",
    "\n",
    "The regression models can be found in `jaxgp.regression`. The `ExactGPR` model takes as inital arguments a kernel to use, initial parameters for the kernel and a noise parameter that underlies the noisy datageneration.\n",
    "\n",
    "Additionally one can also give the model a logging function that saves the parameters at each step of the optimization. For this a convenience class `Logger` can be found in `jaxgp.utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.regression import ExactGPR\n",
    "from jaxgp.utils import Logger\n",
    "\n",
    "logger = Logger()\n",
    "model = ExactGPR(kernel, init_kernel_params, noise, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(X_split, Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterations of the parameters during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logger.iters_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with the trained model\n",
    "\n",
    "We will make predictions on an evenly spaced grid of 200 points in the above defined boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_grid = jnp.linspace(*bounds, 200)\n",
    "\n",
    "# model.eval returns a mean prediction and a confidence interval around the mean prediction\n",
    "means, stds = model.eval(predict_grid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the result\n",
    "\n",
    "Plotting the predicted function together with its confidence interval we can see that even with only a few datapoints the prediction works good. The shaded region describes the $1\\sigma$ confidence interval around the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(predict_grid, means, label=\"prediction\")\n",
    "plt.fill_between(predict_grid, means-stds, means+stds, alpha=0.5)\n",
    "\n",
    "plt.plot(predict_grid, sin(predict_grid), c=\"gray\", ls=\"--\",label=\"true function\")\n",
    "\n",
    "plt.scatter(x_func, y_func, c=\"r\", label=\"function eval\")\n",
    "for i,x in enumerate(x_der): \n",
    "    if i == 0:\n",
    "        plt.axvline(x, c=\"r\", lw=0.8, ls=\"--\", label=\"deriv positions\")\n",
    "    else:\n",
    "        plt.axvline(x, c=\"r\", lw=0.8, ls=\"--\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sparse GPR model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the sparse GPR model\n",
    "\n",
    "The `sparseGPR` model can be found in `jaxgp.regression`. The idea of a sparse model is to project your training data into a space with smaller dimension in order to save in computational cost. This is done by projecting your full training set onto a set of reference points via the kernel. Typically one simply projects the full dataset onto a smaller subset of the existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgp.regression import SparseGPR\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "num_ref_points = (3,)\n",
    "key, subkey = random.split(key)\n",
    "X_ref = random.choice(subkey, x_der, num_ref_points)\n",
    "\n",
    "model = SparseGPR(kernel, init_kernel_params, noise, X_ref, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(X_split, Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterations of the parameters during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logger.iters_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with the trained model\n",
    "\n",
    "We will again make predictions on an evenly spaced grid of 200 points in the above defined boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_grid = jnp.linspace(*bounds, 200)\n",
    "\n",
    "# model.eval returns a mean prediction and a confidence interval around the mean prediction\n",
    "means, stds = model.eval(predict_grid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the result\n",
    "\n",
    "Plotting the predicted function together with its confidence interval we can see that even with only a few datapoints the prediction works good. Close to the reference points the model works similar to the full GPR, however, far away the uncertainty get larger. The shaded region describes the $1\\sigma$ confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(predict_grid, means, label=\"prediction\")\n",
    "plt.fill_between(predict_grid, means-stds, means+stds, alpha=0.5)\n",
    "\n",
    "plt.plot(predict_grid, sin(predict_grid), c=\"gray\", ls=\"--\",label=\"true function\")\n",
    "\n",
    "plt.scatter(x_func, y_func, c=\"r\", label=\"function eval\")\n",
    "for i,x in enumerate(x_der): \n",
    "    if i == 0:\n",
    "        plt.axvline(x, c=\"r\", lw=0.8, ls=\"-\", label=\"deriv positions\")\n",
    "    else:\n",
    "        plt.axvline(x, c=\"r\", lw=0.8, ls=\"-\")\n",
    "\n",
    "for i,x in enumerate(X_ref):\n",
    "    if i == 0:\n",
    "        plt.axvline(x, c=\"k\", lw=0.8, ls=\"--\", label=\"reference positions\")\n",
    "    else:\n",
    "        plt.axvline(x, c=\"k\", lw=0.8, ls=\"--\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing full and sparse GPR to a standard numerical integration via the trapezoidal rule\n",
    "\n",
    "Now we repeat the datageneration as above on an evenly spaced grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data points at which to sample the function and its derivative\n",
    "x_der = jnp.linspace(*bounds, *num_d_vals)\n",
    "x_func = jnp.linspace(*bounds, *num_f_vals)\n",
    "\n",
    "#create training data\n",
    "X_split = [x_func.reshape(-1,1),x_der.reshape(-1,1)]\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "y_func = sin(x_func,noise, subkey)\n",
    "key, subkey = random.split(key)\n",
    "y_der = cos(x_der, noise, subkey)\n",
    "\n",
    "Y_train = jnp.hstack((y_func, y_der))\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "X_ref = random.choice(subkey, x_der, num_ref_points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also train the model again and predict values on the same grid as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExactGPR(kernel, init_kernel_params, noise)\n",
    "model.train(X_split, Y_train)\n",
    "\n",
    "means_full, stds_full = model.eval(predict_grid)\n",
    "\n",
    "model = SparseGPR(kernel, init_kernel_params, noise, X_ref)\n",
    "model.train(X_split, Y_train)\n",
    "\n",
    "means_sparse, stds_sparse = model.eval(predict_grid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the trapezoidal rule to numerically integrate the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trapsum = []\n",
    "for i,elem in enumerate(y_der):\n",
    "    trapsum.append(jnp.sum(y_der[:i+1]) - 0.5*(y_der[0] + y_der[i]))\n",
    "trapsum = jnp.array(trapsum) * (x_der[1] - x_der[0])\n",
    "\n",
    "# Add the function value at zero to fix the integration constant\n",
    "trapsum += y_func"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result for comparison. The shaded regions describe the $1\\sigma$ confidence intervals around the mean prediction.\n",
    "\n",
    "While the accuracy is comparable, the GPR produces a smooth fit and also gives a confidence interval around the prediction.\n",
    "\n",
    "Comparing also the full GPR with the sparse GPR the fitted functions are similar, however, for the sparse version the confidence intervals are larger in between the reference points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predict_grid, means_full, label=\"full prediction\")\n",
    "plt.fill_between(predict_grid, means_full-stds_full, means_full+stds_full, alpha=0.5)\n",
    "\n",
    "plt.plot(predict_grid, means_sparse, label=\"sparse prediction\")\n",
    "plt.fill_between(predict_grid, means_sparse-stds_sparse, means_sparse+stds_sparse, alpha=0.5)\n",
    "\n",
    "plt.plot(x_der,trapsum, label=\"trapezoid\")\n",
    "\n",
    "plt.plot(predict_grid, sin(predict_grid), c=\"gray\", ls=\"--\",label=\"true function\")\n",
    "\n",
    "plt.scatter(x_func, y_func, c=\"r\", label=\"function eval\")\n",
    "for i,x in enumerate(x_der): \n",
    "    if i == 0:\n",
    "        plt.axvline(x, c=\"r\", lw=0.8, ls=\"-\", label=\"deriv positions\")\n",
    "    else:\n",
    "        plt.axvline(x, c=\"r\", lw=0.8, ls=\"-\")\n",
    "\n",
    "for i,x in enumerate(X_ref):\n",
    "    if i == 0:\n",
    "        plt.axvline(x, c=\"k\", lw=0.8, ls=\"--\", label=\"reference positions\")\n",
    "    else:\n",
    "        plt.axvline(x, c=\"k\", lw=0.8, ls=\"--\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
